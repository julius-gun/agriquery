\begin{table}[!htbp]
\caption{Performance of English Embedding Models}
\label{tab:english_embedding_performance}
  \footnotesize
\begin{tabular}{lccccc}
\toprule
Metric & Acc. & F1 & Prec. & Rec. & Spec. \\
LLM &  &  &  &  &  \\
\midrule
\midrule
Gemini 2.5 Flash & \textbf{0.861} & \textbf{0.867} & \textbf{0.831} & \textbf{0.907} & \textbf{0.815} \\
Qwen 2.5 7B & 0.815 & 0.815 & 0.815 & 0.815 & \textbf{0.815} \\
Qwen3 8B & 0.796 & 0.800 & 0.786 & 0.815 & 0.778 \\
Phi3 14B & 0.787 & 0.793 & 0.772 & 0.815 & 0.759 \\
Llama3.1 8B & 0.731 & 0.756 & 0.692 & 0.833 & 0.630 \\
Deepseek-R1 8B & 0.657 & 0.684 & 0.635 & 0.741 & 0.574 \\
Llama3.2 3B & 0.778 & 0.786 & 0.759 & 0.815 & 0.741 \\
Deepseek-R1 1.5B & 0.565 & 0.605 & 0.554 & 0.667 & 0.463 \\
Llama3.2 1B & 0.435 & 0.573 & 0.461 & 0.759 & 0.111 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htbp]
\caption{Performance of French Embedding Models}
\label{tab:french_embedding_performance}
  \footnotesize
\begin{tabular}{lccccc}
\toprule
Metric & Acc. & F1 & Prec. & Rec. & Spec. \\
LLM &  &  &  &  &  \\
\midrule
\midrule
Gemini 2.5 Flash & \textbf{0.806} & \textbf{0.784} & \textbf{0.884} & \textbf{0.704} & \textbf{0.907} \\
Qwen 2.5 7B & 0.741 & 0.714 & 0.795 & 0.648 & 0.833 \\
Qwen3 8B & 0.657 & 0.626 & 0.689 & 0.574 & 0.741 \\
Phi3 14B & 0.648 & 0.642 & 0.654 & 0.630 & 0.667 \\
Llama3.1 8B & 0.704 & 0.704 & 0.704 & \textbf{0.704} & 0.704 \\
Deepseek-R1 8B & 0.537 & 0.545 & 0.536 & 0.556 & 0.519 \\
Llama3.2 3B & 0.648 & 0.548 & 0.767 & 0.426 & 0.870 \\
Deepseek-R1 1.5B & 0.380 & 0.385 & 0.382 & 0.389 & 0.370 \\
Llama3.2 1B & 0.324 & 0.425 & 0.370 & 0.500 & 0.148 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!htbp]
\caption{Performance of German Embedding Models}
\label{tab:german_embedding_performance}
  \footnotesize
\begin{tabular}{lccccc}
\toprule
Metric & Acc. & F1 & Prec. & Rec. & Spec. \\
LLM &  &  &  &  &  \\
\midrule
\midrule
Gemini 2.5 Flash & \textbf{0.824} & \textbf{0.822} & \textbf{0.830} & \textbf{0.815} & \textbf{0.833} \\
Qwen 2.5 7B & 0.759 & 0.740 & 0.804 & 0.685 & \textbf{0.833} \\
Qwen3 8B & 0.694 & 0.692 & 0.698 & 0.685 & 0.704 \\
Phi3 14B & 0.722 & 0.732 & 0.707 & 0.759 & 0.685 \\
Llama3.1 8B & 0.722 & 0.732 & 0.707 & 0.759 & 0.685 \\
Deepseek-R1 8B & 0.537 & 0.583 & 0.530 & 0.648 & 0.426 \\
Llama3.2 3B & 0.713 & 0.674 & 0.780 & 0.593 & \textbf{0.833} \\
Deepseek-R1 1.5B & 0.389 & 0.431 & 0.403 & 0.463 & 0.315 \\
Llama3.2 1B & 0.565 & 0.561 & 0.566 & 0.556 & 0.574 \\
\bottomrule
\end{tabular}
\end{table}